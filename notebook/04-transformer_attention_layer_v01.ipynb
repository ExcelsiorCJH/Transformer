{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer - Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.datamodule import ETTDataModule\n",
    "from src.model.embeddings import DataEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETT DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_params = {\n",
    "    \"data_path\": \"../data/ETT-small/ETTh1.csv\",\n",
    "    \"task\": \"M\",\n",
    "    \"freq\": \"h\",\n",
    "    \"target\": \"OT\",\n",
    "    \"seq_len\": 96,\n",
    "    \"label_len\": 48,\n",
    "    \"pred_len\": 96,\n",
    "    \"use_scaler\": True,\n",
    "    \"use_time_enc\": True,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "\n",
    "dm = ETTDataModule(**dm_params)\n",
    "train_dataloader = dm.train_dataloader()\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_params = {\n",
    "    \"c_in\": 7,\n",
    "    \"d_model\": 512,\n",
    "    \"embed_type\": \"time_features\",\n",
    "    \"freq\": \"h\",\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "embedding = DataEmbedding(**emb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding(x=batch[\"past_values\"], x_features=batch[\"past_time_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 96, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_keys = None\n",
    "d_values = None\n",
    "attention_dropout = 0.1\n",
    "output_attention = False\n",
    "\n",
    "d_keys = d_keys or (d_model // n_heads)\n",
    "d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "\n",
    "query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "dropout = nn.Dropout(attention_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = x\n",
    "keys = x\n",
    "values = x\n",
    "\n",
    "B, L, _ = queries.shape\n",
    "_, S, _ = keys.shape\n",
    "H = n_heads\n",
    "\n",
    "queries = query_projection(queries).reshape(B, L, H, -1)\n",
    "keys = key_projection(keys).reshape(B, S, H, -1)\n",
    "values = value_projection(values).reshape(B, S, H, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = None\n",
    "\n",
    "B, L, H, E = queries.shape\n",
    "_, S, _, D = values.shape\n",
    "scale = scale or 1.0 / sqrt(E)\n",
    "\n",
    "scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = dropout(torch.softmax(scale * scores, dim=-1))\n",
    "V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "V = V.reshape(B, L, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out_projection(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Attention Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_keys: int = None,\n",
    "        d_values: int = None,\n",
    "        scale: float = None,\n",
    "        attention_dropout: float = 0.1,\n",
    "        output_attention: bool = False,\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = scale\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor):\n",
    "        # Q, K, V projection\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        self.scale = self.scale or 1.0 / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        A = self.dropout(torch.softmax(self.scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        out = self.out_projection(V.reshape(B, L, -1))\n",
    "\n",
    "        if self.output_attention:\n",
    "            return out, A\n",
    "        else:\n",
    "            return out, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_params = {\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_keys\": None,\n",
    "    \"d_values\": None,\n",
    "    \"scale\": None,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"output_attention\": True,\n",
    "}\n",
    "\n",
    "attn_layer = Attention(**attn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding(x=batch[\"past_values\"], x_features=batch[\"past_time_features\"])\n",
    "\n",
    "new_x, attn = attn_layer(queries=x, keys=x, values=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 96, 512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 96, 96])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('fedtracker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "095ffe90df4ca297051b97375d5904c113ac48c02dcabb78607607a5f0a97f85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
