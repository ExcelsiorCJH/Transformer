{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import ETTDataModule\n",
    "from src.model import DataEmbedding\n",
    "from src.model import Attention\n",
    "from src.model import Encoder, Decoder\n",
    "from src.model import EncoderLayer, DecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. prev setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_params = {\n",
    "    \"data_path\": \"../data/ETT-small/ETTh1.csv\",\n",
    "    \"task\": \"M\",\n",
    "    \"freq\": \"h\",\n",
    "    \"target\": \"OT\",\n",
    "    \"seq_len\": 96,\n",
    "    \"label_len\": 48,\n",
    "    \"pred_len\": 96,\n",
    "    \"use_scaler\": True,\n",
    "    \"use_time_enc\": True,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "\n",
    "dm = ETTDataModule(**dm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_params = {\n",
    "    \"c_in\": 7,\n",
    "    \"d_model\": 512,\n",
    "    \"embed_type\": \"time_features\",\n",
    "    \"freq\": \"h\",\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "enc_embedding = DataEmbedding(**emb_params)\n",
    "dec_embedding = DataEmbedding(**emb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_params = {\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_keys\": None,\n",
    "    \"d_values\": None,\n",
    "    \"scale\": None,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"output_attention\": True,\n",
    "}\n",
    "\n",
    "attn_layer = Attention(**attn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer_params = {\n",
    "    \"attention\": Attention(**attn_params),\n",
    "    \"d_model\": 512,\n",
    "    \"d_ff\": 2048,\n",
    "    \"dropout\": 0.1,\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "dec_layer_params = {\n",
    "    \"self_attention\": Attention(**attn_params),\n",
    "    \"cross_attention\": Attention(**attn_params),\n",
    "    \"d_model\": 512,\n",
    "    \"d_ff\": 2048,\n",
    "    \"dropout\": 0.1,\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "num_enc_layers: int = 2\n",
    "num_dec_layers: int = 1\n",
    "c_out = 7\n",
    "\n",
    "encoder = Encoder(\n",
    "    enc_layers=[EncoderLayer(**enc_layer_params) for _ in range(num_enc_layers)],\n",
    "    norm_layer=nn.LayerNorm(d_model),\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    dec_layers=[DecoderLayer(**dec_layer_params) for _ in range(num_dec_layers)],\n",
    "    norm_layer=nn.LayerNorm(d_model),\n",
    "    projection=nn.Linear(d_model, c_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_params\n",
    "Config = namedtuple(\n",
    "    \"Config\",\n",
    "    [\n",
    "        \"c_in\",\n",
    "        \"c_out\",\n",
    "        \"d_model\",\n",
    "        \"embed_type\",\n",
    "        \"freq\",\n",
    "        \"dropout\",\n",
    "        \"n_heads\",\n",
    "        \"d_keys\",\n",
    "        \"d_values\",\n",
    "        \"d_ff\",\n",
    "        \"scale\",\n",
    "        \"attention_dropout\",\n",
    "        \"output_attention\",\n",
    "        \"activation\",\n",
    "        \"num_enc_layers\",\n",
    "        \"num_dec_layers\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "configs = Config(\n",
    "    c_in=7,\n",
    "    c_out=7,\n",
    "    d_model=512,\n",
    "    embed_type=\"time_features\",\n",
    "    freq=\"h\",\n",
    "    dropout=0.1,\n",
    "    n_heads=8,\n",
    "    d_keys=None,\n",
    "    d_values=None,\n",
    "    d_ff=2048,\n",
    "    scale=None,\n",
    "    attention_dropout=0.1,\n",
    "    output_attention=True,\n",
    "    activation=\"gelu\",\n",
    "    num_enc_layers=2,\n",
    "    num_dec_layers=1,\n",
    ")\n",
    "\n",
    "# TODO: task_name, pred_len 추가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
