{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "\n",
    "import omegaconf\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import ETTDataModule\n",
    "from src.model import Transformer\n",
    "from src.module.utils import AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_params = {\n",
    "    \"data_path\": \"../data/ETT-small/ETTh1.csv\",\n",
    "    \"task\": \"M\",\n",
    "    \"freq\": \"h\",\n",
    "    \"target\": \"OT\",\n",
    "    \"seq_len\": 96,\n",
    "    \"label_len\": 48,\n",
    "    \"pred_len\": 96,\n",
    "    \"use_scaler\": True,\n",
    "    \"use_time_enc\": True,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "\n",
    "dm = ETTDataModule(**dm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_params\n",
    "Config = namedtuple(\n",
    "    \"Config\",\n",
    "    [\n",
    "        \"task_name\",\n",
    "        \"pred_len\",\n",
    "        \"seq_len\",\n",
    "        \"num_class\",\n",
    "        \"enc_in\",\n",
    "        \"dec_in\",\n",
    "        \"c_out\",\n",
    "        \"d_model\",\n",
    "        \"embed_type\",\n",
    "        \"freq\",\n",
    "        \"dropout\",\n",
    "        \"n_heads\",\n",
    "        \"d_keys\",\n",
    "        \"d_values\",\n",
    "        \"d_ff\",\n",
    "        \"scale\",\n",
    "        \"attention_dropout\",\n",
    "        \"output_attention\",\n",
    "        \"activation\",\n",
    "        \"num_enc_layers\",\n",
    "        \"num_dec_layers\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "configs = Config(\n",
    "    task_name=\"long_term_forecast\",\n",
    "    pred_len=96,\n",
    "    seq_len=None,\n",
    "    num_class=None,\n",
    "    enc_in=7,\n",
    "    dec_in=7,\n",
    "    c_out=7,\n",
    "    d_model=512,\n",
    "    embed_type=\"time_features\",\n",
    "    freq=\"h\",\n",
    "    dropout=0.1,\n",
    "    n_heads=8,\n",
    "    d_keys=None,\n",
    "    d_values=None,\n",
    "    d_ff=2048,\n",
    "    scale=None,\n",
    "    attention_dropout=0.1,\n",
    "    output_attention=True,\n",
    "    activation=\"gelu\",\n",
    "    num_enc_layers=2,\n",
    "    num_dec_layers=1,\n",
    ")\n",
    "\n",
    "model = Transformer(**configs._asdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, dm, config, scaler):\n",
    "        self.config = config\n",
    "        self.nprocs = torch.cuda.device_count()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # model\n",
    "        self.model = model.to(self.device)\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "        self.scaler = scaler\n",
    "\n",
    "        # datamodule(dm)\n",
    "        self.dm = dm\n",
    "        self.train_loader = self.dm.train_dataloader()\n",
    "        self.val_loader = self.dm.val_dataloader()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer, self.lr_scheduler = self.configure_optimizers()\n",
    "\n",
    "        # model-saving options\n",
    "        self.version = 0\n",
    "        self.ckpt_paths = []\n",
    "        while True:\n",
    "            ckpt_dir = self.config.train.ckpt_dir\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.mkdir(ckpt_dir)\n",
    "\n",
    "            self.save_path = os.path.join(\n",
    "                ckpt_dir,\n",
    "                f\"version-{self.config.datamodule.dataset_name}-{self.version}\",\n",
    "            )\n",
    "            if not os.path.exists(self.save_path):\n",
    "                os.makedirs(self.save_path)\n",
    "                break\n",
    "            else:\n",
    "                self.version += 1\n",
    "        self.summarywriter = SummaryWriter(self.save_path)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.global_val_loss = 1e5\n",
    "        self.eval_step = self.config.train.eval_step\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(self.save_path, \"experiment.log\"),\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s > %(message)s\",\n",
    "        )\n",
    "\n",
    "        # experiment-logging options\n",
    "        self.best_result = {\"version\": self.version}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [\n",
    "            {\n",
    "                \"params\": self.model.module.img_encoder.parameters(),\n",
    "                \"lr\": self.config.train.img_encoder_lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": self.model.module.text_encoder.parameters(),\n",
    "                \"lr\": self.config.train.text_encoder_lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": itertools.chain(\n",
    "                    self.model.module.img_projection.parameters(),\n",
    "                    self.model.module.text_projection.parameters(),\n",
    "                ),\n",
    "                \"lr\": self.config.train.proj_head_lr,\n",
    "                \"weight_decay\": self.config.train.weight_decay,\n",
    "            },\n",
    "        ]\n",
    "        # optimizer\n",
    "        optimizer = optim.AdamW(params, weight_decay=0.0)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            patience=self.config.train.patience,\n",
    "            factor=self.config.train.factor,\n",
    "        )\n",
    "        return optimizer, lr_scheduler\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        val_loss: float,\n",
    "        model: nn.Module,\n",
    "    ) -> None:\n",
    "        logging.info(\n",
    "            f\"Val loss decreased ({self.global_val_loss:.4f} â†’ {val_loss:.4f}). Saving model ...\"\n",
    "        )\n",
    "        self.global_val_loss = val_loss\n",
    "\n",
    "        ckpt_path = os.path.join(self.save_path, f\"epoch_{epoch}_{val_loss:.4f}.pt\")\n",
    "\n",
    "        save_top_k = self.config.train.save_top_k\n",
    "        self.ckpt_paths.append(ckpt_path)\n",
    "        if save_top_k < len(self.ckpt_paths):\n",
    "            for path in self.ckpt_paths[:-save_top_k]:\n",
    "                os.remove(path)\n",
    "\n",
    "            self.ckpt_paths = self.ckpt_paths[-save_top_k:]\n",
    "\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "    def fit(self) -> dict:\n",
    "        for epoch in tqdm(range(self.config.train.epochs), desc=\"epoch\"):\n",
    "            logging.info(f\"* Learning Rate: {self.optimizer.param_groups[0]['lr']:.5f}\")\n",
    "            result = self._train_epoch(epoch)\n",
    "\n",
    "            # update checkpoint\n",
    "            if result[\"val_loss\"] < self.global_val_loss:\n",
    "                self.save_checkpoint(epoch, result[\"val_loss\"], self.model)\n",
    "\n",
    "            self.lr_scheduler.step(result[\"val_loss\"])\n",
    "\n",
    "        self.summarywriter.close()\n",
    "        return self.version\n",
    "\n",
    "    def _train_epoch(self, epoch: int) -> dict:\n",
    "        train_loss = AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "        for step, batch in tqdm(\n",
    "            enumerate(self.train_loader),\n",
    "            desc=\"train_steps\",\n",
    "            total=len(self.train_loader),\n",
    "        ):\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items() if k != \"caption\"}\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            if self.config.dp.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(batch)\n",
    "                    loss = outputs[\"loss\"].mean()\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                outputs = self.model(batch)\n",
    "                loss = outputs[\"loss\"].mean()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            train_loss.update(loss.item())\n",
    "\n",
    "            self.global_step += 1\n",
    "            if self.global_step % self.eval_step == 0:\n",
    "                logging.info(\n",
    "                    f\"[DDP Version {self.version} Epoch {epoch}] global step: {self.global_step}, train loss: {loss.item():.3f}\"\n",
    "                )\n",
    "\n",
    "        train_loss = train_loss.avg\n",
    "        val_loss = self.validate(epoch)\n",
    "\n",
    "        # tensorboard writing\n",
    "        self.summarywriter.add_scalars(\n",
    "            \"lr\", {\"lr\": self.optimizer.param_groups[0][\"lr\"]}, epoch\n",
    "        )\n",
    "        self.summarywriter.add_scalars(\n",
    "            \"loss/step\", {\"val\": val_loss, \"train\": train_loss}, self.global_step\n",
    "        )\n",
    "        self.summarywriter.add_scalars(\n",
    "            \"loss/epoch\", {\"val\": val_loss, \"train\": train_loss}, epoch\n",
    "        )\n",
    "\n",
    "        logging.info(f\"** global step: {self.global_step}, val loss: {val_loss:.4f}\")\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def validate(self, epoch: int) -> dict:\n",
    "        val_loss = AverageMeter()\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, batch in tqdm(\n",
    "                enumerate(self.val_loader),\n",
    "                desc=\"valid_steps\",\n",
    "                total=len(self.val_loader),\n",
    "            ):\n",
    "                batch = {\n",
    "                    k: v.to(self.device) for k, v in batch.items() if k != \"caption\"\n",
    "                }\n",
    "\n",
    "                outputs = self.model(batch)\n",
    "                loss = outputs[\"loss\"].mean()\n",
    "                val_loss.update(loss.item())\n",
    "\n",
    "        return val_loss.avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
